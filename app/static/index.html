<!-- app/static/index.html -->
<!DOCTYPE html>
<html>
<head>
    <title>Voice Agent</title>
    <style>
        body { font-family: sans-serif; text-align: center; margin-top: 50px; }
        #status { font-weight: bold; color: gray; }
        button { padding: 15px 30px; font-size: 18px; cursor: pointer; }
    </style>
</head>
<body>
    <h1>üéôÔ∏è Barge-In Voice Agent</h1>
    <p id="status">Disconnected</p>
    <button id="startBtn">Start Conversation</button>

    <script>
        const startBtn = document.getElementById('startBtn');
        const status = document.getElementById('status');
        let ws;
        let audioContext;
        let processor;
        let inputStream;

        startBtn.onclick = async () => {
            startBtn.disabled = true;
            status.innerText = "Connecting...";
            
            // 1. Initialize WebSocket
            ws = new WebSocket(`ws://${location.host}/ws/voice`);
            ws.binaryType = 'arraybuffer';

            ws.onopen = () => {
                status.innerText = "üü¢ Online - Speak now!";
                initAudio();
            };

            ws.onmessage = async (event) => {
                // Play received audio (Bot response)
                playAudio(event.data);
            };
            
            ws.onclose = () => {
                status.innerText = "üî¥ Disconnected";
                startBtn.disabled = false;
                stopAudio();
            };
        };

        async function initAudio() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
            
            // Microphone input
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: { 
                    channelCount: 1, 
                    sampleRate: 16000,
                    echoCancellation: true,
                    noiseSuppression: true
                } 
            });
            inputStream = stream;

            const source = audioContext.createMediaStreamSource(stream);
            // ScriptProcessor is deprecated but easiest for a single-file demo. 
            // Buffer size 4096 = ~0.25s latency
            processor = audioContext.createScriptProcessor(4096, 1, 1);

            processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);
                // Downsample or send as is. We send float32 directly.
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(inputData);
                }
            };

            source.connect(processor);
            processor.connect(audioContext.destination); // Needed for Chrome to activate processor
        }

        function playAudio(arrayBuffer) {
            ws.onmessage = (event) => {
            if (!(event.data instanceof ArrayBuffer)) return;

            const float32 = new Float32Array(event.data);

            const buffer = audioContext.createBuffer(
                1,
                float32.length,
                16000
            );

            buffer.copyToChannel(float32, 0);

            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);
            source.start();
            };
        }

        function stopAudio() {
            if (processor) processor.disconnect();
            if (inputStream) inputStream.getTracks().forEach(track => track.stop());
            if (audioContext) audioContext.close();
        }
    </script>
</body>
</html>